{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from tools import (ego_to_cam, get_only_in_img_mask, denormalize_img, normalize_img,\n",
    "                    SimpleLoss, get_val_info, add_ego, gen_dx_bx,img_transform,\n",
    "                    get_nusc_maps, plot_nusc_map, generate_video_from_imgs)\n",
    "from models import compile_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add paths\n",
    "dataroot = '/home/navya/data/LSS/LSSCAM'\n",
    "map_folder = '/home/navya/data/nuscenes/trainval/'\n",
    "modelf = '/home/navya/project/lift-splat-shoot/model70000.pt'\n",
    "imgname = '/home/navya/project/lift-splat-shoot/input.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sample_augmentation():\n",
    "        H, W = data_aug_conf['H'], data_aug_conf['W']\n",
    "        fH, fW = data_aug_conf['final_dim']\n",
    "        # if self.is_train:\n",
    "        #     resize = np.random.uniform(*self.data_aug_conf['resize_lim'])\n",
    "        #     resize_dims = (int(W*resize), int(H*resize))\n",
    "        #     newW, newH = resize_dims\n",
    "        #     crop_h = int((1 - np.random.uniform(*self.data_aug_conf['bot_pct_lim']))*newH) - fH\n",
    "        #     crop_w = int(np.random.uniform(0, max(0, newW - fW)))\n",
    "        #     crop = (crop_w, crop_h, crop_w + fW, crop_h + fH)\n",
    "        #     flip = False\n",
    "        #     if self.data_aug_conf['rand_flip'] and np.random.choice([0, 1]):\n",
    "        #         flip = True\n",
    "        #     rotate = np.random.uniform(*self.data_aug_conf['rot_lim'])\n",
    "        # else:\n",
    "        resize = max(fH/H, fW/W)\n",
    "        resize_dims = (int(W*resize), int(H*resize))\n",
    "        newW, newH = resize_dims\n",
    "        crop_h = int((1 - np.mean(data_aug_conf['bot_pct_lim']))*newH) - fH\n",
    "        crop_w = int(max(0, newW - fW) / 2)\n",
    "        crop = (crop_w, crop_h, crop_w + fW, crop_h + fH)\n",
    "        flip = False\n",
    "        rotate = 0\n",
    "        return resize, resize_dims, crop, flip, rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "gpuid=0\n",
    "viz_train=False\n",
    "video_output=True\n",
    "max_frames=-1\n",
    "channel=1\n",
    "\n",
    "H=900\n",
    "W=1600\n",
    "resize_lim=(0.193, 0.225)\n",
    "final_dim=(128, 352)\n",
    "bot_pct_lim=(0.0, 0.22)\n",
    "rot_lim=(-5.4, 5.4)\n",
    "rand_flip=True\n",
    "\n",
    "xbound=[-50.0, 50.0, 0.5]\n",
    "ybound=[-50.0, 50.0, 0.5]\n",
    "zbound=[-10.0, 10.0, 20.0]\n",
    "dbound=[4.0, 45.0, 1.0]\n",
    "\n",
    "bsz=1\n",
    "nworkers=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_conf = {\n",
    "    'xbound': xbound,\n",
    "    'ybound': ybound,\n",
    "    'zbound': zbound,\n",
    "    'dbound': dbound,\n",
    "}\n",
    "\n",
    "cams = ['CAM_FRONT']\n",
    "\n",
    "data_aug_conf = {\n",
    "    'resize_lim': resize_lim,\n",
    "    'final_dim': final_dim,\n",
    "    'rot_lim': rot_lim,\n",
    "    'H': H, 'W': W,\n",
    "    'rand_flip': rand_flip,\n",
    "    'bot_pct_lim': bot_pct_lim,\n",
    "    'cams': cams,\n",
    "    'Ncams': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "loading /home/navya/project/lift-splat-shoot/model70000.pt\n"
     ]
    }
   ],
   "source": [
    "nusc_maps = get_nusc_maps(map_folder)\n",
    "\n",
    "device = torch.device('cpu') if gpuid < 0 else torch.device(f'cuda:{gpuid}')\n",
    "\n",
    "model = compile_model(grid_conf, data_aug_conf, outC=3)\n",
    "print('loading', modelf)\n",
    "model.load_state_dict(torch.load(modelf))\n",
    "model.to(device)\n",
    "\n",
    "dx, bx, _ = gen_dx_bx(grid_conf['xbound'], grid_conf['ybound'], grid_conf['zbound'])\n",
    "dx, bx = dx[:2].numpy(), bx[:2].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "val = 0.01\n",
    "fH, fW = final_dim\n",
    "model.eval()\n",
    "counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "img = Image.open(imgname)\n",
    "imgs.append(normalize_img(img))\n",
    "imgs = torch.stack(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "rots = torch.tensor([[ 0.0103,  0.0084,  0.9999],\n",
    "                    [-0.9999,  0.0123,  0.0102],\n",
    "                    [-0.0122, -0.9999,  0.0086]])\n",
    "\n",
    "trans = torch.tensor([ 1.7220,  0.0048,  1.4949])\n",
    "    \n",
    "intrins = torch.tensor([[1.2528e+03, 0.0000e+00, 8.2659e+02],\n",
    "                    [0.0000e+00, 1.2528e+03, 4.6998e+02],\n",
    "                    [0.0000e+00, 0.0000e+00, 1.0000e+00]])\n",
    "    \n",
    "post_rot = [[1., 0., 0.],\n",
    "            [0., 1., 0.],\n",
    "            [0., 0., 1.]]\n",
    "\n",
    "post_tran = [0., 0., 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_81460/571951475.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# augmentation (resize, crop, horizontal flip, rotate)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresize_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrotate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_augmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m img, post_rot2, post_tran2 = img_transform(img, post_rot, post_tran,\n\u001b[0m\u001b[1;32m      4\u001b[0m                                             \u001b[0mresize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                             \u001b[0mresize_dims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresize_dims\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/lift-splat-shoot/src/tools.py\u001b[0m in \u001b[0;36mimg_transform\u001b[0;34m(img, post_rot, post_tran, resize, resize_dims, crop, flip, rotate)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# post-homography transformation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     \u001b[0mpost_rot\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m     \u001b[0mpost_tran\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcrop\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mflip\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'"
     ]
    }
   ],
   "source": [
    "# augmentation (resize, crop, horizontal flip, rotate)\n",
    "resize, resize_dims, crop, flip, rotate = sample_augmentation()\n",
    "img, post_rot2, post_tran2 = img_transform(img, post_rot, post_tran,\n",
    "                                            resize=resize,\n",
    "                                            resize_dims=resize_dims,\n",
    "                                            crop=crop,\n",
    "                                            flip=flip,\n",
    "                                        rotate=rotate,\n",
    "                                            )\n",
    "\n",
    "# for convenience, make augmentation matrices 3x3\n",
    "post_tran = torch.zeros(3)\n",
    "post_rot = torch.eye(3)\n",
    "post_tran[:2] = post_tran2\n",
    "post_rot[:2, :2] = post_rot2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_81460/1834171167.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     out = model(imgs.to(device),\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mrots\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mtrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mintrins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/lss/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/lift-splat-shoot/src/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, rots, trans, intrins, post_rots, post_trans)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintrins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_rots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_trans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_voxels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintrins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_rots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_trans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbevencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/lift-splat-shoot/src/models.py\u001b[0m in \u001b[0;36mget_voxels\u001b[0;34m(self, x, rots, trans, intrins, post_rots, post_trans)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_voxels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintrins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_rots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_trans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m         \u001b[0mgeom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_geometry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintrins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_rots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_trans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cam_feats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/lift-splat-shoot/src/models.py\u001b[0m in \u001b[0;36mget_geometry\u001b[0;34m(self, rots, trans, intrins, post_rots, post_trans)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mReturns\u001b[0m \u001b[0mB\u001b[0m \u001b[0mx\u001b[0m \u001b[0mN\u001b[0m \u001b[0mx\u001b[0m \u001b[0mD\u001b[0m \u001b[0mx\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0mx\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0mx\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \"\"\"\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;31m#points = self.frustum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 1)"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model(imgs.to(device),\n",
    "        rots.to(device),\n",
    "        trans.to(device),\n",
    "        intrins.to(device),\n",
    "        post_rots.to(device),\n",
    "        post_trans.to(device),\n",
    "        )\n",
    "    out = out.sigmoid().cpu()\n",
    "\n",
    "    plt.imshow(out[.channel])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2006520b7ee4c0b8735dd89f87fe6d9b8c0870929f30f436aaef8bc1229d0e96"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('lss': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
